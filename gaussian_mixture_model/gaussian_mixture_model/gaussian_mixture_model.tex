\documentclass{article} [10pt] % Comment this line out
                                                          % if you need a4paper
                                                          % paper

%\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
%\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



%=====================PACKAGES======================
% The following packages can be found on http:\\www.ctan.org
%-----------------graphics related---------------------
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epstopdf}
 \usepackage{epsfig} % for postscript graphics files
\DeclareGraphicsExtensions{.pdf,.eps,.png,.jpg,.mps}
%\usepackage{minibox}
%\usepackage{subfigure}
\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}

%\usetikzlibrary{calc,patterns,decorations.pathmorphing,decorations.markings}
%\usepackage{pgfplots}
\setlength{\unitlength}{1cm}
%-------------------layout related-------------------
%\usepackage{cite}
\usepackage{verbatim}
\usepackage{enumerate}
\setlength\parindent{0pt}
\setlength{\parskip}{.25cm}
%-------------------------Math related-------------------------------
%\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{empheq}
%\usepackage{algorithm2e}
%\usepackage{algorithm} 
%\usepackage{algpseudocode}
%\usepackage{mathrsfs}
\usepackage{amsthm}
\usepackage{cleveref}
%\newtheoremstyle{definition}
\newtheorem{defi}{Definition}
\newtheorem{asmp}{Assumption}
%-------------------------------------------
%\theoremstyle{plain}
\newtheorem{thm}{Theorem}
\newtheorem{lma}{Lemma}
\DeclareMathOperator{\fl}{fl}
\DeclareMathOperator{\card}{card}
% Comment out the next line to get single spacing

%-------------------shorthand commands------------------------
\newcommand{\bs}{\boldsymbol}
\newcommand{\tr}{\text{tr}}

%=====================TITLE======================
\title{Gaussian mixture model}
\author{Zhe Feng}
\date{\today}


\graphicspath{{Figures/}}

\begin{document}
\noindent
\maketitle
\section{Introduction}
Gaussian mixture model (GMM) is a very interesting model and itself has many applications, though outshined more advanced models recently, it still serve as a good base model for clustering and serve as good stepping stone to understand more complicated models such as hidden markoe model and it is also tightly related to expectation maximisation algorithm (EM-algorithm), a family of algorithms that is behand many statistical models.


\section{Problem description}	\label{sec:problem_ddescription}
\subsection{Model form}
GMM can be described by the following formula:
\begin{align} \label{eq:gmm_form1}
\begin{split}
	P(x; \boldsymbol{\tau}, \boldsymbol\mu, \boldsymbol\Sigma) &= \sum_{k=1}^K \tau_k p_k(x; \mu_k, \Sigma_k)\\
	p_k(x; \mu_k, \Sigma_k) &= (2\pi)^{-\frac{n}{2}}\det |\Sigma_k|^{-\frac{1}{2}}\exp\left[-\frac{1}{2}(x-\mu_k)^\top\Sigma_k^{-1}(x-\mu_k)\right]
\end{split}
\end{align}
where
\begin{itemize}
	\item $x\in \mathbb{R}^n$ is the $n$ dimensional random variable observed
	\item $P:\mathbb{R}^n\rightarrow [0, 1]$ is the GMM probability density function (i.e. our model)
	\item $\boldsymbol\mu=\{\mu_1, \mu_2\cdots, \mu_K\}$ and $\mu_k \in \mathbb{R}^n$ is the mean of the $k$th Gaussian component
	\item $\boldsymbol\Sigma=\{\Sigma_1, \Sigma_3\cdots, \Sigma_K\}$ and $\Sigma_k\in\mathbb{R}^{n \times n}$ is the covariance matrix of the $i$th Gaussian component
	\item $\boldsymbol\tau=\{\tau_1,\tau_2,\cdots, \tau_K\}$ and $\tau_k\in\mathbb{R}$ is the weight of the $k$th component and $\sum_k^K\tau_k=1$, it is also called the mixing parameter. The item $\tau$ can be considered as the prior probability of a hidden state $z_k$
\end{itemize}
With this formulation, similar to the 3 fundamental problems of hidden Markov model (HMM), we are interested in solving the following problems:
\begin{enumerate}
	\item\textbf{Learning problem:} given a set of $N$ observations $X=\{x_1, x_2,\cdots,x_N\}$, what is the most likely model $P(\cdot; \boldsymbol{\tau}, \boldsymbol\mu, \boldsymbol\Sigma)$
	\item\textbf{Prediction problem:} given a model $P(\cdot; \boldsymbol{\tau}, \boldsymbol\mu, \boldsymbol\Sigma)$ and an observation $x$, what is the probability of $x$ is generated by the $k$th component?
	\item\textbf{Evaluation problem:} given a set of $N$ observations $X=\{x_1, x_2,\cdots,x_N\}$ and a model $P(\cdot; \boldsymbol{\tau}, \boldsymbol\mu, \boldsymbol\Sigma)$, what is the probability $X$ is generated by the model $P$.
\end{enumerate}
The 3rd problem is probably less interested by people and the the 2nd problem is in general trivial to solve for GMM. We mainly care about the first learning problem.


\subsection{The learning problem and the optimisation problem under the hood}
In terms of learning problem, as stated in the last section, we are given a set of observations $X=\{x_1, x_2\cdots x_N\}$ and we want to find a set of model parameters $\boldsymbol\theta=\{\boldsymbol\tau, \boldsymbol\mu, \boldsymbol\Sigma\}$ such that the likelihood of observing the data set $X$ is maximised. Mathematically:
\begin{align}
	\max_{\boldsymbol\tau, \boldsymbol\mu, \boldsymbol\Sigma} L(\boldsymbol\tau, \boldsymbol\mu, \boldsymbol\Sigma;X) = \prod_{i=1}^N \sum_{k=1}^K \tau_k p_k(x_i; \mu_k, \Sigma_k)
\end{align}

\section{Solve the optimisation problem}
We can of course try to solve the nonlinear optimisation problem with some generic numerical nonlinear programming solvers such as interior point method, but it would be relatively slow and less robust. Instead, people use Expectation-Maximisation (EM) algorithm to solve problem like this.

\subsection{EM-algorithm}
To solve this problem with EM algorithm, we need to reformat the problem \eqref{eq:gmm_form1} a bit. Assume GMM is a generative model with a latent variable $z=\{1, 2\cdots K\}$ indicates which gaussian component is `activated' and the probability of a data point $x$ is generated by the $k$th component is $P(z=k)=\tau_k$, similart to $X$, we can define $Z=\{z_1, z_2\cdots z_N\}$ then the likelihood function can be written as:

\begin{align} \label{eq:complete_log_likelihood}
	 L(\bs\mu, \bs\Sigma;X, Z) = \prod_{i=1}^N \prod_{k=1}^K \left[\tau_k p_k(x_i; \mu_k, \Sigma_k)\right]^{\mathbb{I}(z_i=k)}
\end{align}
where $\mathbb{I}(z_i=k)$ is the indicator function which equals 1 if $z_i=k$ and 0 otherwise. Note the above likelihood can also be expressed as $ \prod_{i=1}^N \sum_{k=1}^K \mathbb{I}(z_i=k)\left[\tau_k p_k(x_i; \mu_k, \Sigma_k)\right]$, but in this way, it will cause trouble in the log-likelihood function, which is:
\begin{align}
	\log L(\bs\tau, \bs\mu, \bs\Sigma;X, Z) &= \log\prod_{i=1}^N \prod_{k=1}^K \left[\tau_k p_k(x_i; \mu_k, \Sigma_k)\right]^{\mathbb{I}(z_i=k)}\\
		&= \sum_{i=1}^N \sum_{k=1}^K \mathbb{I}(z_i=k)\left[\log\tau_k + \log p_k(x_i; \mu_k, \Sigma_k)\right]
\end{align}
To simiplify notation a bit (and also makes it a bit more general), we define model parameters $\theta:=\{\bs\tau, \bs\mu, \bs\Sigma\}$.

Now our optimisation problem become:
\begin{align}
	\max_{\theta}\log L(\theta;X, Z)= \sum_{i=1}^N \sum_{k=1}^K \mathbb{I}(z_i=k)\left[\log\tau_k + \log p_k(x_i; \mu_k, \Sigma_k)\right]
\end{align}

\subsubsection{The E-step}
In EM algorithm, the E-step, or the expectation step is to take the expectation of the log-likelihood function over hiddden variable $Z$, i.e. find $Q(\theta; X):=E_{Z|X}[\log L(\theta;X, Z)]$. Note that the expected log-likelihood function $Q(\theta;X)$ is a function of model parameter $\theta$ parameterised by $X$, not $Z$ as it is averaged over $Z$.
\begin{align}
	Q(\theta;X)&:=E_{Z|X}[\log L(\theta;X, Z)]\\
	&=E_{Z|X}\left[\sum_{i=1}^N \sum_{k=1}^K\mathbb{I}(z_i=k)\left[\log\tau_k + \log p_k(x_i; \mu_k, \Sigma_k)\right]\right]\\
	&=\sum_{i=1}^N E_{z_i|x_i} \left[\sum_{k=1}^K\mathbb{I}(z_i=k)\left[\log\tau_k + \log p_k(x_i; \mu_k, \Sigma_k)\right] \right]\\
	&=\sum_{i=1}^N \sum_{k=1}^K P(z_i=k|x_i)\left[\log\tau_k + \log p_k(x_i; \mu_k, \Sigma_k)\right]
\end{align}
Now we need to calculate the posterior of $z_i$:
\begin{align}
	T_{k, i}:=P(z_i=k|x_i) &= \frac{P(x_i|z_i=k )P(z_i=k) }{P(x_i)}\\
	&=\frac{P(x_i|z_i=k )P(z_i=k) }{\sum_{k=1}^KP(x_i|z_i=k)P(z_i=k)}
\end{align}
substitute $P(x_i|z_i=k )=p_k(x_i;\mu_k,\Sigma_k)$ and $P(z_i=k)=\tau_k$, we have:
\begin{align}\label{eq:expectation_T}
\boxed{
	T_{k, i}=\frac{p_k(x_i;\mu_k,\Sigma_k)\tau_k}{\sum_{k=1}^Kp_k(x_i;\mu_k,\Sigma_k)\tau_k}
}
\end{align}

With the posterior $P(z_i=k|x_i)=T_{k, i}$ calculated, which is the main output of the E-step algorithmically, we can evaluate the expected log-likelihood $Q(\theta|X)$, and this is the objective function in the M-step for maximisation.

\subsubsection{The M-step}
The M-step, or the maximisation step, is to maximize the expected log-likelihood function $Q(\theta; X)$ w.r.t. $\theta$:
\begin{align}
	\theta^* &=\arg\max_\theta Q(\theta; X)=\sum_{i=1}^N \sum_{k=1}^K T_{k, i}\left[\log\tau_k + \log p_k(x_i; \mu_k, \Sigma_k)\right]
\end{align}
recall that $\theta=\{\bs\tau, \bs\mu, \bs\Sigma\}$ and
\begin{align*}
p_k(x_i; \mu_k, \Sigma_k)= (2\pi)^{-\frac{n}{2}}\det |\Sigma_k|^{-\frac{1}{2}}\exp\left[-\frac{1}{2}(x-\mu_k)^\top\Sigma_k^{-1}(x-\mu_k)\right]
\end{align*}
which is the Gaussain distribution. This optimisation problem can be solved analytically by setting the derivative to 0. We first substitute the gaussian pdf in the above equation and we have 
\begin{align*}
Q(\theta|X)&=\sum_{i=1}^N \sum_{k=1}^K T_{k, i}[\log\tau_k + \log (2\pi)^{-\frac{n}{2}}+\log\det |\Sigma_k|^{-\frac{1}{2}}\\
	&-\frac{1}{2}(x_i-\mu_k)^\top\Sigma_k^{-1}(x_i-\mu_k)]\\
\end{align*}

\textbf{Calculate $\bs\tau^*$}\\
To calculate $\tau^*$ we take derivative against $Q$, however with one twist: we need to have the constraint $\sum_k^K\tau_k=1$, so equivaliently we are solving
\begin{align}
	\bs\tau^*, \lambda^* = \arg\max_{\bs\tau, \lambda} \hat{Q}(\bs\tau, \lambda):=\sum_{i=1}^N\sum_{k=1}^KT_{k,i}\log \tau_k + \lambda (1-\sum_{k=1}^K\tau_k)
\end{align}
where $\lambda$ is the Lagrange multiplier. Note that technically speaking we also need the constraint $\tau_k\geq 0$, but this constraint is satisfied automatically since we have $T_{k,i}>0$ and the above equation is linear combination. The above is also not 100\% rigourous because the multiplier shoudl also applied to the original objective function, the only reason we can do this is because other decision variables does not concern $\lambda$. 

The derivative w.r.t. $\tau_k$ is:
\begin{align}
	\frac{\partial }{\partial \tau_k} \hat Q(\tau_k, \lambda) &= \frac{\sum_{i=1}^N T_{k, i}}{\tau_k} + \lambda\\
	\frac{\partial }{\partial \lambda} \hat Q(\tau_k, \lambda) &= 1-\sum_{k=1}^K \tau_k
\end{align}
The optimal solution is obtained at where gradient vanished, thus $\tau_k^*$ and $\lambda^*$
\begin{align}
	0&= \frac{\sum_{i=1}^N T_{k, i}}{\tau_k}^* - \lambda^*\\
	0&= 1-\sum_{j\neq k}^K \tau_j^* + \tau_k^* \label{eq:optimal_lambda_eq}
\end{align}
Note $k$ here is not used for indexing, but represent a specific index (1, 2 etc.) and $j$ is used for indexing sums. From the above we have:
\begin{align}
	\tau_k^* = \frac{\sum_{i=1}^N T_{k, i}}{\lambda^*} \ \forall k \label{eq:optimal_tau_k}
\end{align}
Now we need a separate equation to determine $\lambda^*$, this can be done by substitue equation \eqref{eq:optimal_tau_k} into \eqref{eq:optimal_lambda_eq}:
\begin{align}
	0&= 1-\sum_{k=0}^K  \frac{\sum_{i=1}^N T_{k, i}}{\lambda^*} \\
	\therefore \lambda^* &= \sum_{i=1}^N  \sum_{k=0}^K  T_{k, i} = N
\end{align}
note we used the fact $ \sum_{k=0}^K  T_{k, i} =1$ because $T_{k, i}$ is the posterior for $z_i$ and it sums to 1 along $k$.
Eventually, we have our optimal $\bs\tau^*$:

\begin{align} \label{eq:optimal_tau}
\boxed{
	\bs\tau^* = [\tau_k^*]_{k=1:K} = \frac{1}{N}\sum_{i=1}^N T_{k, i} \ \forall k
}
\end{align}
Interestingly, if we compare the unconstrained solution, which is $\sum_i^NT_{k,i}$, the constrained solution is the normalised version of it. We can also see, that the optimal value (in terms of maximum likelihood estimator) of prior of $z_i$, $\tau_k$ is the expected value of the posterior of $z_i$, $T_{k, i}$ averaged over all observations (This is also essentially the MLE solution for binomial distribution).

\textbf{Calculate $\bs\mu^*$}\\
Following similar principle, taking derivative of $Q$ w.r.t. $\bs\mu$ we have:
\begin{align}
	\frac{\partial }{\partial \mu_k} Q(\mu_k) &=\frac{\partial }{\partial \mu_k} \sum_{i=1}^N \sum_{j=1}^K 
	-T_{j, i}\frac{1}{2}(x_i-\mu_j)^\top\Sigma_k^{-1}(x_i-\mu_j)\\
	&=\sum_{i=1}^N -T_{k, i}\Sigma_k^{-1}(x_i-\mu_k)
\end{align}
agian, we use $k$ here only for a specific component and use $j$ as the sum index. By setting derivative to 0 we have:
\begin{align}
	0=\sum_{i=1}^N -T_{k, i}\Sigma_k^{-1}(x-\mu_k)\\
	 \Sigma_k^{-1}\sum_{i=1}^N  T_{k, i} \mu_k =\Sigma_k^{-1}\sum_{i=1}^NT_{k, i}x_i
\end{align}
therefore by rearange the above equation, we have our optimal $\bs\mu^*$:
\begin{align} \label{eq:optimal_mu}
\boxed{
	\bs\mu^*=[\mu_k^*]_{k=1:K} = \frac{\sum_{i=1}^NT_{k, i}x_i}{\sum_{i=1}^N  T_{k, i}} \ \forall k
}
\end{align}
Compare to the maximum likelihood estimation of mean for gaussian distribution $\frac{1}{N}\sum_i^Nx_i$, the above estimate is equivalent of a `soft count' or a `weighted count' w.r.t. to the posterior probability of $z_i$ version of it.


\textbf{Calculate $\bs\Sigma^*$}\\
The estimation of the optimal covariance matrix $\bs\Sigma^*$ is the trickest part. First we will need some property about matrices, the following equalities are from fundamental linear algebra and we will not derive them here, but use them as they are:
\begin{itemize}
	\item property of trace (cyclic product's trace are the same):
	\begin{align}
		\tr[ABC]  = \tr[BCA] = \tr[CAB]
	\end{align}	
	\item derivative of trace (similar to derivative of linear combination):
	\begin{align}
		\frac{\partial}{\partial A}  \tr[BA]  = B^\top
	\end{align}
	\item derivative of quadratic form (used the above to properties):
	\begin{align}\label{eq:A_derivative_of_quadratic_form}
		\frac{\partial}{\partial A} x^\top A x = \frac{\partial}{\partial A} \tr[x^\top A x]=\frac{\partial}{\partial A} \tr[xx^\top A ] = [xx^\top]^\top=xx^\top
	\end{align}
	\item derivative of log of determinant:
	\begin{align} \label{eq:A_derivative_of_log_det}
		\frac{\partial}{\partial A}\log\det|A| = (A^\top)^{-1} = A^{-1}
	\end{align}
	where $A$ is a symmetric matrix
\end{itemize}
Now if we take derivative of Q w.r.t. $\bs\Sigma$:
\begin{align}
\frac{\partial}{\partial\Sigma_k}Q(\Sigma_k)&=\frac{\partial\Sigma_k^{-1}}{\partial\Sigma_k}\frac{\partial}{\partial\Sigma_k^{-1}}\sum_{i=1}^N \sum_{j=1}^K T_{k, i}[ \log\det |\Sigma_k|^{-\frac{1}{2}}-\frac{1}{2}(x_i-\mu_k)^\top\Sigma_k^{-1}(x_i-\mu_k)]\\
&=\frac{\partial\Sigma_k^{-1}}{\partial\Sigma_k} \frac{\partial}{\partial\Sigma_k^{-1}}\sum_{i=1}^N T_{k, i}[ \frac{1}{2}\log\det |\Sigma_k|^{-1}-\frac{1}{2}(x_i-\mu_k)^\top\Sigma_k^{-1}(x_i-\mu_k)]\\
\end{align}
Now with equation \eqref{eq:A_derivative_of_quadratic_form} for quadratic form and equation \eqref{eq:A_derivative_of_log_det} for log of determinant, we have:
\begin{align}
\frac{\partial}{\partial\Sigma_k}Q(\Sigma_k)&=\frac{\partial\Sigma_k^{-1}}{\partial\Sigma_k} \frac{\partial}{\partial\Sigma_k^{-1}}\sum_{i=1}^N T_{k, i}[ \frac{1}{2}\log\det |\Sigma_k|^{-1}-\frac{1}{2}(x_i-\mu_k)^\top\Sigma_k^{-1}(x_i-\mu_k)]\\
&=\frac{\partial\Sigma_k^{-1}}{\partial\Sigma_k} \sum_{i=1}^N T_{k, i}\frac{1}{2}\Sigma_k -  T_{k, i}\frac{1}{2}(x_i-\mu_k)(x_i-\mu_k)^\top
\end{align}
Note that $(x_i-\mu_k)(x_i-\mu_k)^\top\in\mathbb{R}^{n\times n}$ is the outer product, also we only caculated $\frac{\partial Q(\Sigma_k)}{\partial\Sigma_k^{-1}}$ and left $\frac{\partial\Sigma_k^{-1}}{\partial\Sigma_k}$ as it is, because when setting derivative to 0, this term will cancel out:
\begin{align}
0=\frac{\partial\Sigma_k^{-1}}{\partial\Sigma_k} \sum_{i=1}^N T_{k, i}\frac{1}{2}\Sigma_k -  T_{k, i}\frac{1}{2}(x_i-\mu_k)(x_i-\mu_k)^\top\\
0=\frac{\partial\Sigma_k^{-1}}{\partial\Sigma_k} \sum_{i=1}^N T_{k, i}\frac{1}{2}\Sigma_k - \frac{\partial\Sigma_k^{-1}}{\partial\Sigma_k} \sum_{i=1}^N T_{k, i}\frac{1}{2}(x_i-\mu_k)(x_i-\mu_k)^\top\\
\frac{\partial\Sigma_k^{-1}}{\partial\Sigma_k} \sum_{i=1}^N T_{k, i}\frac{1}{2}\Sigma_k=  \frac{\partial\Sigma_k^{-1}}{\partial\Sigma_k} \sum_{i=1}^N T_{k, i}\frac{1}{2}(x_i-\mu_k)(x_i-\mu_k)^\top\\
\end{align}
Now we can see $\frac{\partial\Sigma_k^{-1}}{\partial\Sigma_k}$ can be cancelled out on both sides. The optimal $\bs\Sigma$ is then:
\begin{align}
\boxed{
	\bs\Sigma^*=[\Sigma_k]_{k=1:K}=\frac{\sum_{i=1}^N T_{k, i}(x_i-\mu_k)(x_i-\mu_k)^\top}{ \sum_{i=1}^N T_{k, i}}
}
\end{align}
\subsection{EM-algorithm for computing Gaussiam Mixture Model}
In summary, the algorithm can be summarised as the following:
\begin{enumerate}
	\item Initialise with random $\bs\tau$, $\bs\mu$ and $\bs\Sigma$
	\item\label{e-step} take E-step, calculate $T_{k, i}$:
		\begin{align}
		\boxed{
			T_{k, i}=\frac{p_k(x_i;\mu_k,\Sigma_k)\tau_k}{\sum_{k=1}^Kp_k(x_i;\mu_k,\Sigma_k)\tau_k}
		}
		\end{align}
	\item\label{m-step}  take M-step, update $\bs\tau$, $\bs\mu$ and $\bs\Sigma$:
		\begin{align}
			\bs\tau^* &= [\tau_k^*]_{k=1:K} = \frac{1}{N}\sum_{i=1}^N T_{k, i} \ \forall k\\
			\bs\mu^*&=[\mu_k^*]_{k=1:K} = \frac{\sum_{i=1}^NT_{k, i}x_i}{\sum_{i=1}^N  T_{k, i}} \ \forall k\\
			\bs\Sigma^*&=[\Sigma_k]_{k=1:K}=\frac{\sum_{i=1}^N T_{k, i}(x_i-\mu_k)(x_i-\mu_k)^\top}{ \sum_{i=1}^N T_{k, i}}
		\end{align}
	\item repeat from step \ref{e-step} and  \ref{m-step} until converge.
\end{enumerate}
%\subsection{Implementation consideration}
%In general, computing use matrix from is more convenient than computing with loops. When implementing the EM algorithm, it's helpful to define few more quantities and re-write the above in matrix form.
%
%In E-step, when calculating posterior $T_{k,i}$, it's natural to define a matrix $T$ with $k, i$-th component as $T_{k,i}$. Define the component probability vector of the gaussian components as
%\begin{align}
%	p(x; \bs\mu, \bs\Sigma) := [p_1(x; \mu_1, \Sigma_1), p_2(x; \mu_2, \Sigma_2),\cdots, p_K(x; \mu_K, \Sigma_K)]\in[0, 1]^K
%\end{align}
%and when apply to feature matrix $X\in\mathbb{R}^{N\times n}$ we have 
%\begin{align}
%	p(X; \bs\mu, \bs\Sigma) = \begin{bmatrix}
%		p(x_1; \bs\mu, \bs\Sigma)\\
%		p(x_2; \bs\mu, \bs\Sigma)\\
%		\vdots \\
%		p(x_N; \bs\mu, \bs\Sigma)
%	\end{bmatrix}\in[0, 1]^{N\times K}
%\end{align}
%We also define the $\bs\tau$ in matrix form and repeat over vertical axis as:
%\begin{align}
%	[\bs\tau]_{N} = \begin{bmatrix}
%		\tau_1 & \tau_2 & \cdots & \tau_K \\
%		\tau_1 & \tau_2 & \cdots & \tau_K \\
%		& \vdots\\
%	\end{bmatrix}\in[0, 1]^{N\times K}
%\end{align}
%
%then the equation \eqref{eq:expectation_T} for calculating optimal $T_{k,i}$ can be written as:
%\begin{align}
% p(X; \bs\mu, \bs\Sigma)
%\end{align}
%

\newpage




\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

































